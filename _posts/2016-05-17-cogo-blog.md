---
title: 'Choosing the right tool for the analyst'
layout: default
---
# Choosing the right tool for the analyst

At Cogo, we get the opportunity to use some of the newest technologies around. 

When working on creating the next web businesses, analysts usually have to be the project pioneers.
We'll have to explore an idea, build the first iteration of it, and try to get a working version up to be evaluated as quickly as possible.

Because our engineering team only provides Cogo-wide support, we have much more technical responsibility to see our projects come to fruition.
As a result, we get exposed to a much broader set of technologies than your typical data analyst.

Here's a taste of some tools that our analysts use when tackling their day-to-day tasks.


### Data management

The first question for any data analysis problem is- "Where does the data live? How is it stored?"

Every analyst comes in and learns [SQL](https://en.wikipedia.org/wiki/SQL) (if they don't already know it).

SQL is only possible to use once the data is set up properly.
There is still the problem of how to organize and store the data in the first place.

While we have a dedicated data team at Cogo for managing our biggest systems, there is a large responsibility within each analyst team to manage some of our own data. 
We've found that this enables individual analyst teams to move as fast as possible through the problems at hand.
The more experienced engineers will step in to help when the problem has either become too complex, or is generalizable to several teams.


The most common way for us to store and organize data is to use some type of [relational database management system, or RDBMS](https://en.wikipedia.org/wiki/Relational_database_management_system). 
Most often we use [MySQL](https://www.mysql.com/), [Postgres](http://www.postgresql.org/), or [Amazon Redshift](http://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html).
Occasionally we will just use [Amazon's S3 service](https://aws.amazon.com/s3/) if the data is very large and doesn't need to be as structured and queryable.

How do we decide which one of these to use? 
[Since each one has it's own advantages and disadvantages](https://www.digitalocean.com/community/tutorials/sqlite-vs-mysql-vs-postgresql-a-comparison-of-relational-database-management-systems), it is largely up to the individual analyst teams to pick based on preference.

Usually Redshift comes in once the data set is quite large (approaching a billion rows), as this is the point when [Redshift can prove to outperform Postgres (or MySQL) for many types of queries](https://www.periscopedata.com/blog/redshift-and-rds-postgres-benchmarked.html).
Since Redshift data management is more involved, our engineering team helps the [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) and cluster management of our Redshift data.


### Analysis Tools

Once our data has been uploaded, nicely formatted, and readied for use, what tools do our analysts use?

#### Python, the powerhouse

By far the most common answer would be [Python](https://www.python.org/).
We have this as the default tool all analysts need to learn once they arrive at Cogo for three reasons:

1. Ease of learning
2. Quick turnaround for ideas
3. Ability to scale to most of the problems we need

Due to the flexibility and popularity of the [NumPy](http://www.numpy.org/), [SciPy](https://www.scipy.org/), [pandas](http://pandas.pydata.org/), and [scikit-learn](http://scikit-learn.org/) packages, Python is becoming the industry standard for data analysis.

Additionally, learning Python sets our analysts up for interacting with the bigger engineering systems with Cogo, many of which are also written in Python.

#### R

The other popular data analysis tool used less frequently around the building is [R](https://www.r-project.org/about.html).
Since it is the [other standard tool for data analysis](https://www.datacamp.com/community/tutorials/r-or-python-for-data-analysis), many people, especially those coming from academia, arrive with a fair amount of experience in R.

R is useful to the data scientists within Cogo because of the huge library of statistical functions available, as well as the more mature plotting that comes from [ggplot](http://www.statmethods.net/advgraphs/ggplot2.html).

Because Python is a much more general tool and goes far beyond just Pandas and SciPy, we have analysts learn Python over R.
However, we never discourage people from deriving clear, presentable results with whatever tool works.


#### Excel

For the quickest analysis work, plenty of analsts also use Excel. 
It is hard to beat the speed of opening your CSV file, making a quick manipulation in a pivot table, then having a presentable graph in seconds.
Especially for the analysts who come in very familiar with Excel, there will almost always be a place to have the nice graphical interface for moving around your data.


### Data, but bigger

Once in awhile, an analyst might need to find some answer based on many months of historical data.
If this becomes too much for traditional relational databases to handle in one query, they may use tools designed to handle larger quantities of data of one time.
Most commonly, we will write either a [MapReduce](https://en.wikipedia.org/wiki/MapReduce) or, more recently, a [Spark](http://spark.apache.org/) job.

While our engineering team has set up some infrastructure to run [Hadoop](http://hadoop.apache.org/) jobs on our own machines, we have been moving more and more to [Amazon's Elastic MapReduce](https://aws.amazon.com/elasticmapreduce/) web service.
This pairs nicely with our S3 and Redshift usage, allowing minimal overhead for transfering data from one tool to another.

Because analysts use Python far more than any other language, we will usually use either [MrJob](https://pythonhosted.org/mrjob/) or [PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html).
These Python libraries greatly aid development speed by allowing the job to be written purely in Python (as opposed Java).

Some of our engineers have also recently begun experimenting with [presto](https://prestodb.io/) for a few of our big databases. 
This has been especially nice for analysts who would like to access our largest databases, but don't want to write an entire map reduce job for every answer.

### In House Specialties

In addition to al lthe open source technology we use, our Eengineering team has created two incredible tools for sharing data analyses.
The first is **Quake**- which stands for Query Utility And Knowledge Engine.
Quake provides a website where you can create, run, and share SQL executions on any of our dozens of databases.

The other tool, called **à la chart**, is a service that provides a drop-down menu for creating a beautiful chart from any SQL excution that ran on Quake.
The underlying library to à la chart is [C3.js](http://c3js.org/), which has a variety of standard ways to visualize data.

Having these tools in our arsenal means that when we discuss and present analyses at board meetings, we aren't just glossing over our methodoligies.
We are sharing the SQL queries, graphs, and code to all who want to poke at, play with, or extend the analysis work.
It turns what was used to be a black box of results into a fully collaborative environment.

---------

-------

-------

------

Possible ways to divide this page:

- How we pick a tool for database management, analysis tasks, infrastructure, deployment
- Where we use python, where we use sql, where we use mr, ....
- How we approach a single problem at hand (do it in python? do it in sql? in map reduce?)

What we consider

- development time:
  - do i need this fast?

- run speed:
  - will this thing finish?

- simplicity and understandability
  - will someone coming to help be able to figure out whats going on?

