---
title: 'Choosing the right tool for the analyst'
layout: post
categories: articles
redirect_from:
- /tutorials/2016/05/17/cogo/blog
- /articles/2016/05/17/cogo/blog
---
# Choosing the right tool for the analyst

When creating the next web businesses at Cogo Labs, Analysts are often the project pioneers. When we have new ideas we want to explore, we have to try to get a working version up as quickly as possible so that it can be evaluated more effectively. 

This means Analysts have significantly more technical responsibility that they might at other big-data companies. Because it’s our responsibility to see our projects come to fruition, we require a much broader set of technologies than your typical data analyst.

Here's a taste of some tools that we use when tackling our day-to-day tasks.


### Data management

The first question for any data analysis problem is- "Where does the data live? How is it stored?"


The most common way for us to store and organize data is to use some type of [relational database management system, or RDBMS](https://en.wikipedia.org/wiki/Relational_database_management_system). 
Most often we use [MySQL](https://www.mysql.com/), [Postgres](http://www.postgresql.org/), or [Amazon Redshift](http://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html).
Occasionally we will just use Amazon’s S3 service if the data is very large and doesn’t need to be as structured and query-able.

How do we decide which one of these to use? 
For smaller projects, the decision is between MySQL and Postgres.
[Each one has its own advantages and disadvantages](https://www.digitalocean.com/community/tutorials/sqlite-vs-mysql-vs-postgresql-a-comparison-of-relational-database-management-systems), so the decision is mostly based how familiar the team is with each variant.


Usually Redshift comes in once the data set is quite large (approaching a billion rows), as this is the point when [Redshift can prove to outperform Postgres (or MySQL) for many types of queries](https://www.periscopedata.com/blog/redshift-and-rds-postgres-benchmarked.html).
Since Redshift data management is more involved, our engineering team helps the [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) and cluster management of our Redshift data.


### Analysis Tools

Once our data has been uploaded, nicely formatted, and readied for use, what tools do our analysts use to do their number crunching?

#### SQL

If they don’t already know it, every Analyst who comes to Cogo learns [SQL](https://en.wikipedia.org/wiki/SQL).
SQL is the language needed to extract data from our databases, so it is the most common thing written by all analysts.

Although you can perform [some pretty complicated statistics](https://www.periscopedata.com/blog/how-to-calculate-confidence-intervals-in-SQL.html) in SQL alone, it's often more trouble than it's worth.
The much more common approach is to extract the data in the format you want, then pass it to the next tool in the pipeline. 

#### Python, the powerhouse

By far the most common next tool would be [Python](https://www.python.org/).
We have this as the default tool all analysts need to learn once they arrive at Cogo for three reasons:

1. Ease of learning
2. Quick turnaround for ideas
3. Ability to scale to most of the problems we need

Due to the flexibility and popularity of the [NumPy](http://www.numpy.org/), [SciPy](https://www.scipy.org/), [pandas](http://pandas.pydata.org/), and [scikit-learn](http://scikit-learn.org/) packages, Python is becoming the industry standard for data analysis.

Additionally, learning Python sets our analysts up for interacting with the bigger engineering systems with Cogo, many of which are also written in Python.

#### R

The other popular data analysis tool used less frequently around the building is [R](https://www.r-project.org/about.html).
Since it is the [other standard tool for data analysis](https://www.datacamp.com/community/tutorials/r-or-python-for-data-analysis), many people, especially those coming from academia, arrive with a fair amount of experience in R.

R is useful to the data scientists within Cogo because of the huge library of statistical functions available, as well as the more mature plotting that comes from [ggplot](http://www.statmethods.net/advgraphs/ggplot2.html).

Because Python is a much more general tool and goes far beyond just Pandas and SciPy, we have analysts learn Python over R.
However, we never discourage people from deriving clear, presentable results with whatever tool works.


#### Excel

For the quickest analysis work, plenty of analysts also use Excel. 
It is hard to beat the speed of opening your CSV file, making a quick manipulation in a pivot table, then having a presentable graph in seconds.
Especially for the analysts who come in very familiar with Excel, there will almost always be a place to have the nice graphical interface for moving around your data.


### Data, but bigger

Once in a while, an analyst might need to find some answer based on many months of historical data.
If this becomes too much for traditional relational databases to handle in one query, they may use tools designed to handle larger quantities of data of one time.
Most commonly, we will write either a [MapReduce](https://en.wikipedia.org/wiki/MapReduce) or, more recently, a [Spark](http://spark.apache.org/) job.

While our engineering team has set up some infrastructure to run [Hadoop](http://hadoop.apache.org/) jobs on our own machines, we have been moving more and more to [Amazon's Elastic MapReduce](https://aws.amazon.com/elasticmapreduce/) web service.
This pairs nicely with our [S3](https://aws.amazon.com/s3/) and Redshift usage, allowing minimal overhead for transferring data from one tool to another.

Some of our engineers have also recently begun working with [Presto](https://prestodb.io/) for a few of our big databases. 
This has been especially nice for analysts who would like to access our largest databases, but don't want to write an entire MapReduce job for every answer.

### In-House Specialties

In addition to all the open source technology we use, our Engineering team has created two incredible tools for sharing data analyses.
The first is **Quake**, which stands for Query Utility And Knowledge Engine.
Quake provides a website where you can create, run, and share SQL executions on any of our dozens of databases.

The other tool, called **à la chart**, is a service that provides a drop-down menu for creating a beautiful chart from any SQL execution that ran on Quake.
The underlying library to à la chart is [C3.js](http://c3js.org/), which has a variety of standard ways to visualize data.

Having these tools in our arsenal means that when we discuss and present analyses, we aren't just glossing over our methodologies.
We are sharing the SQL queries, graphs, and code to all who want to poke at, play with, or extend the analysis work.
It transforms what was used to be a black box of results into a fully collaborative environment.

