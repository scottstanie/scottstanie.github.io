---
category: articles
created: 2025-01-24T08:45
tags:
- llms
- research
- ai
title: Chatting with o1 circa Jan. 2025 - Getting schooled
---

See Chatting with Claude circa November 2024 - A smart colleague emerges

I just had a conversation with ChatGPT o1 which left me somewhere between awe and worry (Others have called this an ["Oh Fuck" moment](https://ghuntley.com/oh-fuck/)).

Before the conversation, I had a little experiment in my head about InSAR, which I was pretty sure would prove my intuition correct. I finally started to code it up last night. Then, before I got to the actual analysis stage, I got an unexpected linear algebra error, 3 steps before I thought I've be plotting results, showing that the premise of this though experiment is wrong.

I really didn't understand. So I pasted the 30 lines of Python console along with the general question "I'm pretty sure this should work, why isn't this? Is it impossible?"

The LLM not only managed to understand my question, not only gave a short intuitive answer why it wouldn't work- it gave a mathematical proof why it wouldn't work using a theorem I had never heard of.

I've already noted that for the past few months, the top LLMs have gone from nice, useful coding helper, to being able to tutor me in my research areas that took 5+ years to get up to speed with. There's still a lot of noise with trying to get out knowledge at that post-graduate level, but I found it incredible that you could *ever* get this machine to teach you new things at what is supposed to be "frontier of knowledge"[^1].

This is the first time it taught me something and that made me feel like I was back in college. 
That is, I had a non-trivial question, but it answered it, and also pointed out to an entirely new (but closely related) set of advanced material from which my question was an obvious consequence.

It was only last week where I read the article on [post-feudal society from having an abundance of machine intelligence](https://nosetgauge.substack.com/p/capital-agi-and-human-ambition) (and [here](https://www.astralcodexten.com/p/its-still-easier-to-imagine-the-end)), which I found interesting, compelling at times, but wildly off at others. Specifically, the author made a remark about how "progress in the hard sciences become all by machines in a few years". I heard "few years" and thought "ok that's such a wrong timeline that now I don't know about the rest of this article".
Now a week later, I have to update my timeline.

[^1]: Much like LLM capabilities, research areas have a jagged frontier (note to the substack guy)... research areas are not created equal. See all the Steven Boyd jokes during his lectures about "backwater field" reinventing an optimization algorithm that they've been using for a decade. Mean, but true.

