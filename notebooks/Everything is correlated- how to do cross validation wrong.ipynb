{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything is correlated\n",
    "Or, how to do cross validation wrong.\n",
    "\n",
    "\n",
    "Let's try a classification problem with a small data set with many possible features.\n",
    "The labels are going to be random, so we know there should be no predictive power to any model we fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoints = 100\n",
    "nfeatures = int(5e4)\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "X = rng.random(size=(npoints, nfeatures));\n",
    "Y = rng.random(size=(npoints,)) > 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pretend that we feel there are too many features, and many are probably useless. We only want the top 100 for making our model.\n",
    "\n",
    "How can we pick these? Let's take the top 100 that have the highest correlation with our labels as the \"useful\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features(X, Y, n=100):\n",
    "    corrs = np.zeros(X.shape[1])\n",
    "    for ii in range(X.shape[1]):\n",
    "        corrs[ii] = np.corrcoef(X[:, ii], Y)[0, 1]\n",
    "    top_idxs = np.argsort(np.abs(corrs))[-n:]\n",
    "    return top_idxs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.05 s, sys: 23.4 ms, total: 4.08 s\n",
      "Wall time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_idxs = select_best_features(X, Y, 100)\n",
    "X100 = X[:, top_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try fitting a model with the trimmed data set, and see how it performs in cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X100, Y, test_size=0.4, random_state=0)\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation scores:\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "folds = 5\n",
    "scores = cross_val_score(clf, X100, Y, cv=folds)\n",
    "print(f\"{folds}-fold cross validation scores:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect cross validation?\n",
    "\n",
    "The model appears to be able to fit the data perfectly... how?\n",
    "\n",
    "What happens if we get the hold out set of X and Y *before* picking the best features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X shape: (60, 50000)\n",
      "output shape: (60, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "print(f\"Input X shape: {X_train.shape}\")\n",
    "top_idxs = select_best_features(X_train, y_train, 100)\n",
    "X100 = X_train[:, top_idxs]\n",
    "print(f\"output shape: {X100.shape}\")\n",
    "\n",
    "# This will be what we check with, after training with the other rows\n",
    "X_holdout = X_test[:, top_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on holdout set: 0.525\n",
      "5-fold cross validation scores:\n",
      "[0.125 0.625 0.625 0.5   0.5  ]\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1).fit(X100, y_train)\n",
    "\n",
    "print(f\"Score on holdout set: {clf.score(X_holdout, y_test)}\")\n",
    "scores = cross_val_score(clf, X_holdout, y_test, cv=5)\n",
    "print(f\"{folds}-fold cross validation scores:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see how the feature selection is *part of the model training*. If we don't set aside the validation set, we won't get a representative score from cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
